# 04-prompt-engineering

import os
from openai import OpenAI
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Initialize OpenAI client with API key
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def call_prompt(prompt: str) -> str:
    """
    Sends a prompt to the GPT-3.5-turbo model and returns the generated response.
    
    Args:
        prompt (str): The input prompt to be sent to the model.
    
    Returns:
        str: The response generated by the model, stripped of leading and trailing whitespace.
    """
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.4
    )
    return response.choices[0].message.content.strip()

prompt_variants = [
    "How can I reduce Docker image size?",
    "What are some best practices to make Docker images smaller?",
    "As a DevOps expert, how would you optimize a Dockerfile to minimize image size?",
    "Explain how to debug large Docker images and reduce unnecessary layers.",
]

if __name__ == "__main__":
    """
    Main function to execute the prompt variants and print the responses.
    This function iterates over a list of prompt variants, sends each to the model,
    and prints the responses in a formatted manner.
    It is designed to demonstrate the effectiveness of different prompt styles
    in eliciting useful information from the model.
    """
    print("ðŸ’¬ Question: How can I reduce Docker image size?\n")
    for i, prompt in enumerate(prompt_variants, start=1):
        print(f"""ðŸ”¹ Prompt Variant {i}: {prompt}
""")
        answer = call_prompt(prompt)
        print(f"""ðŸ§  Response:
{answer}
""")
        print("â€”" * 40 + "\n")