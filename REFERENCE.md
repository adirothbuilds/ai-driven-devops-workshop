# Repository References

## 01-llm-basics/basic_prompt_call.py

### def ask_gpt(prompt: str, model="gpt-3.5-turbo") -> str

```python
"""
Simple command-line interface to ask DevOps-related questions to GPT.
This script allows users to input a question and receive a response from the GPT model.
It is designed to assist with DevOps tasks and queries, providing quick answers 
and insights based on the input question.

Usage:
    Run the script and enter your DevOps question when prompted. The GPT model 
    will generate a response based on the input.

Example:
    $ python basic_prompt_call.py
    ðŸ”§ Enter your DevOps question: How do I set up a CI/CD pipeline?
"""
```

## 02-log-summarization/summarize_log.py

### def summarize_text(text: str, model="gpt-3.5-turbo", max_tokens=400) -> str

```python
"""
Summarizes a given system log text, highlighting errors, warnings, and 
important events using an AI language model.

Args:
    text (str): The system log text to be summarized.
    model (str, optional): The name of the AI model to use for summarization. 
        Defaults to "gpt-3.5-turbo".
    max_tokens (int, optional): The maximum number of tokens for the 
        generated summary. Defaults to 400.

Returns:
    str: A summarized version of the input text, focusing on key details 
    such as errors, warnings, and significant events.
"""
```

### def read_file(file_path: str) -> str

```python
"""
Reads the content of a file and returns it as a string.

Args:
    file_path (str): The path to the file to be read.

Returns:
    str: The content of the file.

Raises:
    FileNotFoundError: If the file does not exist at the specified path.
"""
```

## 05-rag-with-dev-docs/ask_with_context.py

### def load_context(file_path: str) -> str

```python
"""
Loads the content of a file and returns it as a string.

Args:
    file_path (str): The path to the file to be read.

Returns:
    str: The content of the file as a string.

Raises:
    FileNotFoundError: If the file does not exist.
    IOError: If there is an error reading the file.
"""
```

### def ask_question_with_context(context: str, question: str) -> str

```python
"""
Generates a response to a user's question based on provided documentation context.

Args:
    context (str): The documentation or context to use for answering the question.
    question (str): The user's question to be answered.

Returns:
    str: The response generated based on the provided context and question.
"""
```

## 06-ci-integration/summarize_ci_log.py

### def load_log(file_path: str) -> str

```python
"""
Reads the content of a log file and returns it as a string.

Args:
    file_path (str): The path to the log file to be read.

Returns:
    str: The content of the log file as a string.

Raises:
    FileNotFoundError: If the specified file does not exist.
    IOError: If there is an error reading the file.
"""
```

### def summarize_log(log: str) -> str

```python
"""
Summarizes a CI log output, focusing on errors, warnings, and failed tests.
This function uses a conversational AI model to analyze the provided CI log 
and generate a concise summary. The summary highlights key issues, such as 
errors, warnings, or failed tests, and provides insights into what might 
have gone wrong.

Args:
    log (str): The CI log output as a string.

Returns:
    str: A summarized description of the CI log, focusing on critical issues.
"""
```

## 07-cli-agent-demo/devops_agent.py

### def main()

```python
"""
Entry point for the DevOps GPT CLI Agent.
This function initializes a conversation history with a system message and 
enters a loop to interact with the user. Users can input DevOps-related 
questions, and the function will generate responses using a GPT model. 
The conversation history is maintained to provide context for the responses.
The user can exit the loop by typing 'exit' or 'quit'. The function also 
handles keyboard interruptions and unexpected errors gracefully.

Raises:
    Exception: If an unexpected error occurs during the execution.
"""
```

## 08-tool-execution/dockerfile_agent.py

### def save_interaction(user_input, response)

```python
"""
Saves the interaction between the user and the system to a memory file.
This function records the user's input and the system's response by appending
them to a JSON file. If the file or its parent directories do not exist, they
are created. If the file contains invalid JSON, it is ignored, and a new history
is started.

Args:
    user_input (str): The input provided by the user.
    response (str): The response generated by the system.

Raises:
    OSError: If there is an issue creating directories or writing to the file.
"""
```

### def generate_dockerfile(prompt)

```python
"""
Generates a Dockerfile based on the provided prompt.
This function uses an AI model to create a Dockerfile tailored to the user's
requirements as described in the prompt.

Args:
    prompt (str): A natural language description of the desired Dockerfile.

Returns:
    str: The generated Dockerfile content.
"""
```
