# 01-llm-basics

import os
from openai import OpenAI
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Initialize OpenAI client
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def ask_gpt(prompt: str, model="gpt-3.5-turbo") -> str:
    """
    Sends a prompt to the OpenAI GPT model and returns the response.
    
    Args:
        prompt (str): The input prompt to send to the GPT model.
        model (str, optional): The model to use for generating the response. 
            Defaults to "gpt-3.5-turbo".
    Returns:
        str: The response content generated by the GPT model.
    """

    response = client.chat.completions.create(
        model=model,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.4
    )
    return response.choices[0].message.content.strip()

# Example usage
if __name__ == "__main__":
    """
    Simple command-line interface to ask DevOps-related questions to GPT.
    This script allows users to input a question and receive a response from the GPT model.
    It is designed to assist with DevOps tasks and queries, providing quick answers 
    and insights based on the input question.
    
    Usage:
        Run the script and enter your DevOps question when prompted. The GPT model 
        will generate a response based on the input.
        
    Example:
        $ python basic_prompt_call.py
        ðŸ”§ Enter your DevOps question: How do I set up a CI/CD pipeline?
        ðŸ¤– GPT Says: To set up a CI/CD pipeline, you can follow these steps...
    """
    print("== DevOps GPT Helper ==")
    question = input("ðŸ”§ Enter your DevOps question: ")
    answer = ask_gpt(question)
    print("\nðŸ¤– GPT Says:\n", answer)
